---
title: "SparkRDD - SparkR extension for using Spark RDD"
author: "Koji MAKIYAMA (@hoxo_m)"
output: 
  html_document:
    keep_md: true
---

```{r, echo=FALSE }
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Quick Start

[Quick Start - Spark 1.4.0 Documentation](https://spark.apache.org/docs/latest/quick-start.html)

### Create Spark Context

```{r}
library(SparkRDD)
suppressMessages(sc <- sparkR.init(master="local"))
```

### Basics

```{r}
file <- file.path(Sys.getenv("SPARK_HOME"), "README.md")
textFile <- sc %>% textFile(file) 

textFile %>% count
```

```{r}
textFile %>% first
```

```{r}
lineWithSpark <- textFile %>% 
  filterRDD_(line: grepl("Spark", line))

lineWithSpark %>% count
```

### More on RDD Operations

```{r}
textFile %>% 
  map_(line: length(strsplit(line, " ")[[1]])) %>%
  reduce_(a, b: ifelse(a > b, a, b))
```

```{r}
textFile %>% 
  map_(line: length(strsplit(line, " ")[[1]])) %>%
  reduce_(max)
```

```{r}
wordCounts <- textFile %>% 
  flatMap_(line: strsplit(line, " ")[[1]]) %>%
  map_(word: list(word, 1)) %>%
  reduceByKey_(a, b: a + b, numPartitions=1)

wordCounts %>% collect %>% 
  Map_(x: data.frame(word=x[[1]], count=x[[2]])) %>%
  Reduce_(rbind) %>%
  head
```

### Stop

```{r}
sparkR.stop()
```

